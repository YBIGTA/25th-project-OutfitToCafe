{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytvXxY3LoQtY",
        "outputId": "02409a24-a760-4007-d431-ceea23e67418"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdXYWEPmoYHA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_uynjFdiR8c"
      },
      "outputs": [],
      "source": [
        "ha = [\"NB_train\", \"NB_val\",\"NB_test\"]\n",
        "dataset_folder = []\n",
        "for i in ha:\n",
        "    dataset_folder.append(f\"/content/drive/MyDrive/YBIGTA 신입플/Datasets/{i}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xALxO2I7mpAB"
      },
      "source": [
        "We start testing our data augmentations on our simple custom CNN model that will hopefully train quickly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzGcszG0moN_",
        "outputId": "e82a1d73-f164-4adf-e1f3-9b98301dc599"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8DLhfyfmvbg"
      },
      "outputs": [],
      "source": [
        "# I want to start importing the base packages that we need to train our CNN\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import DataLoader, random_split, Subset\n",
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DL7LH1PIm3Xd"
      },
      "outputs": [],
      "source": [
        "# We'll start training with a smaller version of the data to see how CNN performs\n",
        "# Base Model: No Data Augmentations\n",
        "base_transform = {\n",
        "    'train': transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize((224,224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize((224,224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ]),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Blurring\n",
        "base_transform = {\n",
        "    'train': transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.GaussianBlur(kernel_size=(5, 5), sigma=(0.1, 2.0)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize((224,224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize((224,224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ]),\n",
        "}"
      ],
      "metadata": {
        "id": "gCScRYn7trl-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mntFSg2FjNva"
      },
      "outputs": [],
      "source": [
        "# Color Jittering + Blurring\n",
        "additional_transform =  {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ColorJitter(\n",
        "            brightness=0.05,\n",
        "            contrast=0.05,\n",
        "            saturation=0.05,\n",
        "            hue=0.01\n",
        "        ),\n",
        "        transforms.GaussianBlur(kernel_size=(5, 5), sigma=(0.1, 2.0)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize((224,224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize((224,224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ]),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding Affine Transformations + Blurring\n",
        "affine_transform =  {\n",
        "   'train': transforms.Compose([\n",
        "       transforms.Resize((224,224)),\n",
        "       transforms.RandomAffine(\n",
        "           degrees=5,\n",
        "           translate=(0.02, 0.02),\n",
        "           scale=(0.98, 1.02),\n",
        "           shear=5),\n",
        "       transforms.GaussianBlur(kernel_size=(5,5), sigma=(0.1, 2.0)),\n",
        "       transforms.ToTensor(),\n",
        "       transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "   ]),\n",
        "   'val': transforms.Compose([\n",
        "       transforms.Resize((224,224)),\n",
        "       transforms.ToTensor(),\n",
        "       transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "   ]),\n",
        "   'test': transforms.Compose([\n",
        "       transforms.Resize((224,224)),\n",
        "       transforms.ToTensor(),\n",
        "       transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "   ]),\n",
        "}\n"
      ],
      "metadata": {
        "id": "ngqZrPSy6sZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_and_save(dataset_folder, save_folder):\n",
        "    if not os.path.exists(save_folder):\n",
        "        os.makedirs(save_folder)\n",
        "\n",
        "    for subdir, _, files in os.walk(dataset_folder):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(subdir, file)\n",
        "            img = Image.open(file_path).convert(\"RGB\")  # Convert to RGB\n",
        "            img = base_transform[\"train\"](img)\n",
        "            save_path = os.path.join(save_folder, os.path.relpath(subdir, dataset_folder), file + '.pt')\n",
        "            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "            torch.save(img, save_path)\n"
      ],
      "metadata": {
        "id": "O-fyqW7QUy7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "raDJF8m4t6pa"
      },
      "outputs": [],
      "source": [
        "preprocess_and_save(dataset_folder[0], \"/content/drive/MyDrive/YBIGTA 신입플/TransformedDatasets/base_NB_train\")\n",
        "preprocess_and_save(dataset_folder[1], \"/content/drive/MyDrive/YBIGTA 신입플/TransformedDatasets/NB_val\")\n",
        "preprocess_and_save(dataset_folder[2], \"/content/drive/MyDrive/YBIGTA 신입플/TransformedDatasets/NB_test\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_and_saved(dataset_folder, save_folder):\n",
        "    if not os.path.exists(save_folder):\n",
        "        os.makedirs(save_folder)\n",
        "\n",
        "    for subdir, _, files in os.walk(dataset_folder):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(subdir, file)\n",
        "            img = Image.open(file_path).convert(\"RGB\")  # Convert to RGB\n",
        "            img = blur_transform[\"train\"](img)\n",
        "            save_path = os.path.join(save_folder, os.path.relpath(subdir, dataset_folder), file + '.pt')\n",
        "            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "            torch.save(img, save_path)"
      ],
      "metadata": {
        "id": "3u2z9jdu2_Yt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_and_saved(dataset_folder[0],\"/content/drive/MyDrive/YBIGTA 신입플/TransformedDatasets/blur_NB_train_b6\")"
      ],
      "metadata": {
        "id": "aDSzJBPpUns8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1ugQaZkgGew"
      },
      "source": [
        "I ran this code multiple times with slight adjustments to create custom models. The model that is created below is used on a dataset that is resized to 128 x 128, but later on, when we trained our EfficientNet, we had to readjust those sizes 224 x 224. Please keep this in mind if you have any intention of using this code later on!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIclz7PfuULU"
      },
      "source": [
        "Baseline Model \\\\\n",
        "- Custom\n",
        "- A model that performed well with ImageNet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6CRD0cHOmeui"
      },
      "outputs": [],
      "source": [
        "class CustomCNN(nn.Module):\n",
        "    def __init__(self, num_classes: int = 12):\n",
        "        super(CustomCNN, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.fc1_input_size = 128 * 16 * 16\n",
        "\n",
        "        self.fc1 = nn.Linear(self.fc1_input_size, 512)\n",
        "        self.fc2 = nn.Linear(512, num_classes)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "num_classes = 12\n",
        "base_model = CustomCNN(num_classes = num_classes)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "base_model = base_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efz1x8QYnpFg"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(base_model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNeKVbGOplAh"
      },
      "outputs": [],
      "source": [
        "model_save_path = \"/content/drive/MyDrive/YBIGTA/Models\"\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(model_save_path, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7B38Ulu91awR"
      },
      "outputs": [],
      "source": [
        "class PreprocessedDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, folder):\n",
        "        self.file_paths = [os.path.join(dp, f) for dp, dn, filenames in os.walk(folder) for f in filenames]\n",
        "        self.classes = sorted(set(os.path.basename(os.path.dirname(fp)) for fp in self.file_paths))\n",
        "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = torch.load(self.file_paths[idx])\n",
        "        label = self.get_label_from_path(self.file_paths[idx])\n",
        "        return image, label\n",
        "\n",
        "    def get_label_from_path(self, path):\n",
        "        # Get the class name (folder name) from the path\n",
        "        class_name = os.path.basename(os.path.dirname(path))\n",
        "        # Map the class name to an integer label\n",
        "        return self.class_to_idx[class_name]\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "base_train_dataset = PreprocessedDataset(\"/content/drive/MyDrive/YBIGTA 신입플/TransformedDatasets/base_NB_train\")\n",
        "blur_train_dataset = PreprocessedDataset(\"/content/drive/MyDrive/YBIGTA 신입플/TransformedDatasets/blur_NB_train\")\n",
        "additional_train_dataset = PreprocessedDataset(\"/content/drive/MyDrive/YBIGTA 신입플/TransformedDatasets/additional_NB_train\")\n",
        "val_dataset = PreprocessedDataset(\"/content/drive/MyDrive/YBIGTA 신입플/TransformedDatasets/NB_val\")\n",
        "affine_train_dataset = PreprocessedDataset(\"/content/drive/MyDrive/YBIGTA 신입플/TransformedDatasets/Affine_NB_train\")\n",
        "test_dataset = PreprocessedDataset(\"/content/drive/MyDrive/YBIGTA 신입플/TransformedDatasets/NB_test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REBs4Tg51kn1"
      },
      "outputs": [],
      "source": [
        "base_train_loader = DataLoader(base_train_dataset, batch_size=32, shuffle=True, num_workers=1)\n",
        "blur_train_loader = DataLoader(blur_train_dataset, batch_size=32, shuffle=True, num_workers=1)\n",
        "affine_train_loader = DataLoader(affine_train_dataset, batch_size=32, shuffle=True, num_workers=1)\n",
        "additional_train_loader = DataLoader(additional_train_dataset, batch_size=32, shuffle=True, num_workers=1)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=1)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2eVvQPzewFy"
      },
      "outputs": [],
      "source": [
        "blur_dataloaders = {'train': blur_train_loader, 'val': val_loader, \"test\": test_loader}\n",
        "blur_image_datasets = {'train': blur_train_dataset, 'val': val_dataset, \"test\": test_dataset}\n",
        "affine_dataloaders = {'train': affine_train_loader, 'val': val_loader, \"test\": test_loader}\n",
        "affine_image_datasets = {'train': affine_train_dataset, 'val': val_dataset, \"test\": test_dataset}\n",
        "additional_dataloaders = {'train': additional_train_loader, 'val': val_loader, \"test\": test_loader}\n",
        "additional_image_datasets = {'train': additional_train_dataset, 'val': val_dataset, \"test\": test_dataset}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9pxjSDC2I_r"
      },
      "outputs": [],
      "source": [
        "base_dataloaders = {'train': base_train_loader, 'val': val_loader, \"test\": test_loader}\n",
        "base_image_datasets = {'train': base_train_dataset, 'val': val_dataset, \"test\": test_dataset}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ihgsKuhkfzF3"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the code used to train the individual models that were tested below. Some were retested, which is why one of block of codes might not show output!"
      ],
      "metadata": {
        "id": "zxdK9GYtulJz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cwCZ7kst1Fd",
        "outputId": "df383f25-025a-489a-e2e4-834f5ec85ac0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "Training Loss: 2.4309 Acc: 0.1463\n",
            "iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiValidation Loss: 2.2265 Acc: 0.2244\n",
            "Epoch 2/10\n",
            "Training Loss: 2.2143 Acc: 0.2252\n",
            "iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiValidation Loss: 2.1160 Acc: 0.2561\n",
            "Epoch 3/10\n",
            "Training Loss: 2.0898 Acc: 0.2687\n",
            "iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiValidation Loss: 2.0376 Acc: 0.2722\n",
            "Epoch 4/10\n",
            "Training Loss: 1.9764 Acc: 0.3096\n",
            "iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiValidation Loss: 1.9928 Acc: 0.2989\n",
            "Epoch 5/10\n",
            "Training Loss: 1.8666 Acc: 0.3458\n",
            "iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiValidation Loss: 1.9829 Acc: 0.2978\n",
            "Epoch 6/10\n",
            "Training Loss: 1.7432 Acc: 0.3943\n",
            "iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiValidation Loss: 2.0549 Acc: 0.3000\n",
            "Epoch 7/10\n",
            "Training Loss: 1.6260 Acc: 0.4432\n",
            "iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiValidation Loss: 2.0832 Acc: 0.3044\n",
            "Epoch 8/10\n",
            "Training Loss: 1.4961 Acc: 0.4774\n",
            "iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiValidation Loss: 2.1068 Acc: 0.2994\n",
            "Epoch 9/10\n",
            "Training Loss: 1.3693 Acc: 0.5354\n",
            "iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiValidation Loss: 2.3173 Acc: 0.2856\n",
            "Epoch 10/10\n",
            "Training Loss: 1.2096 Acc: 0.5888\n",
            "iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiValidation Loss: 2.3873 Acc: 0.2767\n",
            "Model saved to /content/drive/MyDrive/YBIGTA/Models/affine_augment_custom_model.pth\n"
          ]
        }
      ],
      "source": [
        "import gc\n",
        "\n",
        "num_epochs = 10\n",
        "accumulation_steps = 4\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "\n",
        "    base_model.train()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    for i, (inputs, labels) in enumerate(affine_dataloaders['train']):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        outputs = base_model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        if (i + 1) % accumulation_steps == 0:\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        del inputs, labels, outputs, preds, loss\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    epoch_loss = running_loss / len(affine_image_datasets['train'])\n",
        "    epoch_acc = running_corrects.double() / len(affine_image_datasets['train'])\n",
        "    print(f'Training Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "    base_model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_corrects = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in affine_dataloaders['val']:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            print(\"i\",end=\"\")\n",
        "\n",
        "            outputs = base_model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item() * inputs.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            val_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            del inputs, labels, outputs, preds, loss\n",
        "            gc.collect()\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    val_epoch_loss = val_loss / len(affine_image_datasets['val'])\n",
        "    val_epoch_acc = val_corrects.double() / len(affine_image_datasets['val'])\n",
        "    print(f'Validation Loss: {val_epoch_loss:.4f} Acc: {val_epoch_acc:.4f}')\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "model_save_path = \"/content/drive/MyDrive/YBIGTA/Models/affine_augment_custom_model.pth\"\n",
        "torch.save(base_model.state_dict(), model_save_path)\n",
        "print(f\"Model saved to {model_save_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKHCKHnwTgZM"
      },
      "source": [
        "Testing the Model that were created by loading them down below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1LVdy8L9bjiV"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_save_path = \"/content/drive/MyDrive/YBIGTA 신입플/Models/base_augment_custom_model.pth\"\n",
        "base_model.load_state_dict(torch.load(model_save_path))\n",
        "base_model = base_model.to(device)\n",
        "\n",
        "base_model.eval()\n",
        "\n",
        "running_loss = 0.0\n",
        "running_corrects_top1 = 0\n",
        "running_corrects_top2 = 0\n",
        "running_corrects_top3 = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in base_dataloaders['test']:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        print(\"H\", end=\"\")\n",
        "\n",
        "        outputs = base_model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        _, top3_preds = torch.topk(outputs, 3, dim=1)\n",
        "\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        running_corrects_top1 += torch.sum(preds == labels.data)\n",
        "\n",
        "        correct_top2 = top3_preds[:, :2].eq(labels.view(-1, 1).expand_as(top3_preds[:, :2]))\n",
        "        running_corrects_top2 += torch.sum(correct_top2.any(dim=1))\n",
        "\n",
        "        correct_top3 = top3_preds.eq(labels.view(-1, 1).expand_as(top3_preds))\n",
        "        running_corrects_top3 += torch.sum(correct_top3.any(dim=1))\n",
        "\n",
        "\n",
        "test_loss = running_loss / len(base_image_datasets['test'])\n",
        "test_acc_top1 = running_corrects_top1.double() / len(base_image_datasets['test'])\n",
        "test_acc_top2 = running_corrects_top2.double() / len(base_image_datasets['test'])\n",
        "test_acc_top3 = running_corrects_top3.double() / len(base_image_datasets['test'])\n",
        "print(\"finally\")\n",
        "\n",
        "print(f'Test Loss: {test_loss:.4f}')\n",
        "print(f'Top-1 Acc: {test_acc_top1:.4f}')\n",
        "print(f'Top-2 Acc: {test_acc_top2:.4f}')\n",
        "print(f'Top-3 Acc: {test_acc_top3:.4f}')"
      ],
      "metadata": {
        "id": "8QMpoes6tZpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6wxzGHRZRgJ",
        "outputId": "e0cd25bf-18f9-4c5d-8bdf-14678b99a09d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loss: 2.4773\n",
            "Top-1 Acc: 0.3174\n",
            "Top-2 Acc: 0.4940\n",
            "Top-3 Acc: 0.6188\n"
          ]
        }
      ],
      "source": [
        "model_save_path = \"/content/drive/MyDrive/YBIGTA/Models/blur_augment_custom_model.pth\"\n",
        "base_model.load_state_dict(torch.load(model_save_path))\n",
        "base_model = base_model.to(device)\n",
        "\n",
        "base_model.eval()\n",
        "\n",
        "running_loss = 0.0\n",
        "running_corrects_top1 = 0\n",
        "running_corrects_top2 = 0\n",
        "running_corrects_top3 = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in base_dataloaders['test']:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        outputs = base_model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        _, top3_preds = torch.topk(outputs, 3, dim=1)\n",
        "\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        running_corrects_top1 += torch.sum(preds == labels.data)\n",
        "\n",
        "        correct_top2 = top3_preds[:, :2].eq(labels.view(-1, 1).expand_as(top3_preds[:, :2]))\n",
        "        running_corrects_top2 += torch.sum(correct_top2.any(dim=1))\n",
        "\n",
        "        correct_top3 = top3_preds.eq(labels.view(-1, 1).expand_as(top3_preds))\n",
        "        running_corrects_top3 += torch.sum(correct_top3.any(dim=1))\n",
        "\n",
        "test_loss = running_loss / len(base_image_datasets['test'])\n",
        "test_acc_top1 = running_corrects_top1.double() / len(base_image_datasets['test'])\n",
        "test_acc_top2 = running_corrects_top2.double() / len(base_image_datasets['test'])\n",
        "test_acc_top3 = running_corrects_top3.double() / len(base_image_datasets['test'])\n",
        "\n",
        "print(f'Test Loss: {test_loss:.4f}')\n",
        "print(f'Top-1 Acc: {test_acc_top1:.4f}')\n",
        "print(f'Top-2 Acc: {test_acc_top2:.4f}')\n",
        "print(f'Top-3 Acc: {test_acc_top3:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orSg3a_fIr4O",
        "outputId": "689ab765-54f3-4612-f245-7b596a000a55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loss: 12.3173\n",
            "Top-1 Acc: 0.1048\n",
            "Top-2 Acc: 0.2006\n",
            "Top-3 Acc: 0.2581\n"
          ]
        }
      ],
      "source": [
        "model_save_path = \"/content/drive/MyDrive/YBIGTA/Models/additional_augment_custom_model.pth\"\n",
        "base_model.load_state_dict(torch.load(model_save_path))\n",
        "base_model = base_model.to(device)\n",
        "\n",
        "base_model.eval()\n",
        "\n",
        "running_loss = 0.0\n",
        "running_corrects_top1 = 0\n",
        "running_corrects_top2 = 0\n",
        "running_corrects_top3 = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in base_dataloaders['test']:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        outputs = base_model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        _, top3_preds = torch.topk(outputs, 3, dim=1)\n",
        "\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        running_corrects_top1 += torch.sum(preds == labels.data)\n",
        "\n",
        "        correct_top2 = top3_preds[:, :2].eq(labels.view(-1, 1).expand_as(top3_preds[:, :2]))\n",
        "        running_corrects_top2 += torch.sum(correct_top2.any(dim=1))\n",
        "\n",
        "        correct_top3 = top3_preds.eq(labels.view(-1, 1).expand_as(top3_preds))\n",
        "        running_corrects_top3 += torch.sum(correct_top3.any(dim=1))\n",
        "\n",
        "test_loss = running_loss / len(base_image_datasets['test'])\n",
        "test_acc_top1 = running_corrects_top1.double() / len(base_image_datasets['test'])\n",
        "test_acc_top2 = running_corrects_top2.double() / len(base_image_datasets['test'])\n",
        "test_acc_top3 = running_corrects_top3.double() / len(base_image_datasets['test'])\n",
        "\n",
        "print(f'Test Loss: {test_loss:.4f}')\n",
        "print(f'Top-1 Acc: {test_acc_top1:.4f}')\n",
        "print(f'Top-2 Acc: {test_acc_top2:.4f}')\n",
        "print(f'Top-3 Acc: {test_acc_top3:.4f}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}